{
    "collab_server" : "",
    "contents" : "###############################\n###############################\n# Author: Yaseen Lotfi        #\n# Date: 4/10/2017             #\n# Title: Lab 1 Skeleton Files #\n# Description:                #\n#   Working with real data    #\n###############################\n###############################\n\n# Always include these statements at the top of your scripts.\n# First, a statement to clean your R environment\n# Second, a statement which sets your working directory\n# Third, check that your working directory was set to the proper destination\n\nrm(list = ls())\nsetwd(\"C:/Users/yalot/OneDrive/CompSci/GitHub/Data-Science-in-Space-and-Time/Lab-01-Introduction\")\ngetwd()\n\n##############\n## Packages ##\n##############\n\n# If you need to install the packages:\n#   1) Highlight the install.packages block\n#   2) Ctr/Command + Shift + C to uncomment\n#   3) Run those lines to install\n\n# install.packages(\"dplyr\", dependencies = T)\n# install.packages(\"stringr\", dependencies = T)\n# install.packages(\"ggplot2\", dependencies = T)\n# install.packages(\"ggmap\", dependencies = T)\n# install.packages(\"rgdal\", dependencies = T)\n# install.packages(\"raster\", dependencies = T)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(rgdal)\nlibrary(raster)\n\n# You have now loaded all third-party software meaning\n# that you can now use commands that are otherwise not\n# available in base R.\n\n\n#########################\n### Import Flat Files ###\n#########################\n\n# After installing and loading R software packages,\n# we can start working with our dataset.\n\n# read() commands search for files and create R dataframes.\n# You need to supply the filepath to the dataset\n# To see other options, run ?read.csv in the console.\n\n# The file path begins from your working directory => Look in the data folder and find the *.csv file\n# Tell R how to read this data, splitting each data point everytimme it finds a comma\n#   *.csv == \"comma-separated-value\"\n\ninst_raw_data <- read.csv2(\"data/hd2014.csv\", sep = \",\")\nadmin_raw_data <- read.csv(\"data/adm2014.csv\", sep = \",\")\n\n# What command and arguments would you use if the data was tab-delimited?\n\n############################\n### Merge Two Dataframes ###\n############################\n\n# Sometimes separate data contain information we need for our analysis.\n# In this case it is useful to merge or join two datasets by a common column.\n# Below are two ways to do the same thing. Left join is more precise in what we are\n# actually doing, and this command comes from the \"dplyr\" package. There are different\n# types of join which we could also if we wanted.  Run ?left_join to see these options\n# and read some of the resources on the GitHub Repo for more in depth explanations.\n\nraw_data_merge <- merge(x = inst_raw_data, y = admin_raw_data, by = \"UNITID\", all.x = TRUE)\nraw_data  <- left_join(x = inst_raw_data, y = admin_raw_data, by = \"UNITID\")\n\n# Up to this point, we have created a lot of unnecessary objects so it is a good time\n# clean up our environment a little. After creating a joined dataset, we no longer need\n# the initial datasets. It is a good habit to remove unnecessary objects from your\n# environment from an efficiency standpoint but also from a readability and organization one.\n\nrm(inst_raw_data, admin_raw_data, raw_data_merge)\n\n# Note, if you try to remove an object that does not exist, R will ignore it and only output a warning.\n\n#############################\n### Subsetting & Indexing ###\n#############################\n\n# Now that we aggragated our raw_data, we can extract the features that we are interested\n# in using for our analysis. For the purposes of this exercise, we will not get into what\n# each column means. Instead focus on the mechanics of how we extract the information we want.\n# In the future, you will want to think about how to slice and dice your data.\n\n# STEP 2: SUBSETTING\n# The next two lines are straightforward. We want to pull a subset of the raw data only where\n# the feature ICLEVEL is equal to 9. This command will reduce the number of rows in our data.\n\ninstitutions <- subset(raw_data, HLOFFER == 9)\n\n# STEP 1: INDEXING\n# Indexing is extremely useful. While we take a manual approach here, in the future you will\n# use conditional logic to subset and index. For now, all we are doing is pulling which columns\n# we are interested in. In this instance, we are reducing the number of features in our dataset\n# and saving it in a new data frame called institution. \n\n# Note the syntax of an index:\n\n# objName[rowNumber, colNumber]  # This will return a single value at this position (think cell addresses in excel)\n\n# This syntax is very useful. We could assign a new value at that index position:\n\n# objName[rowNumber, colNumber] <- someValue\n\n# If we are only interested in rows or columns, we can still use indexing as follow:\n# This will return a row-wise or column-wise vector, or 1D list of values. This can be very useful as well,\n# but you must recognize what you are indexing.\n\n# objName[, colNumber]  OR  objName[rowNumber, ]\n\n# You will probably not know the row or column numbers more oftent than not. In this case, we can use\n# conditional logic with the following syntax:\n\n# objName[conditionalTestRow, conditionalTestCol]  # You can include one or both tests.\n\n# There are a lot of way to construct a conditional test, so it is worth looking through some\n# of the resources in the main GitHub repository. We will also emphasize when we use it\n# in the future so you can learn to recognize this tool and build an intuition for it.\n\n# Finally, we happen to no which columns we want to keep, so we will index by column\n# and save the new dataframe in a variable called 'institutions'. Notice we are overwriting\n# the object we already created. This is okay as long as you are aware of what you are doing.\n# A lot of errors in the future might come frome the coder unintentionally overwriting a data\n# object without realizing it. In this case, we are sure we want to do this.\n\ninstitutions <- institutions[,c(1:2,4:5,66:67, 80, 86, 92, seq(118, 144, 2))]\n\n# In this use of indexing, we are listing column numbers that we want to extract.\n# The colon operator simply says, \"1 to 2\" or 66:67. The seq() command will start\n# at the first argument, 118 and count up to the second argument, 144, incrementing by 2.\n# To understand the code, copy seq(118, 144, 2) into your console to see exactly what it is doing.\n\n# Now we have reduced the dimensionality of our data to something relevant to our analysis. We can\n# create new columns by passing some computation over other columns. In this case, we will divide the\n# number of students admitted by the number of applicants and save the admission rate to a new columns\n# we define as 'percent_admit'.\n\ninstitutions$percent_admit <- institutions$ADMSSN/institutions$APPLCN\n\n# The $ operator is very important here.\n\n########################\n### Explore the Data ###\n########################\n\n# Run the following commands in the R Console because they are \n# not necessary for our script. While RStudio will show a lot\n# of this info, you can save the output into variables and\n# use them to execute based on their values. Conditional logic\n# is an important concept beyond the scope of this Lab.\n\n# 1) To view the column names of your dataset, run:\n#   names(objName)  # Replace \"objName\" with name of dataframe\n\n# One way we would use names(x) is for renaming columns. For example,\n# we can rename the first column to \"ID\" with following code:\n\nnames(raw_data)[1]\nnames(raw_data)[1] <- \"ID\"  # Tells R to assign the string value, \"ID\" to the first element of names(raw_data)\nnames(raw_data)[1]\n\n# Note what the old name was and whether or not it changed.\n\n# 2) Show the number of rows and columns, run:\n#   nrow(objName)\n#   ncol(objName)\n\n# 3) Avoid printing all of your data by looking at the first or last n-number of rows\n#   head(objName, n)\n#   tail(objName, n)\n\n# The default is 6 rows. It is more productive to use head or tail than to click on\n# on the data object in the RStudio IDE. Get in the habit of using the console because\n# you will become a more efficient and productive programmer.\n\n# 4) View Data Structure because you should always be thinking of the data type\n#   str(objName)\n\n# 5) Summary Statistics of each variable in your dataser:\n#   summary(objName)\n\n# 5) Is there missing Data? We can count the number of missing data by column.\n\napply(raw_data, 2, function(x) sum(is.na(x)))\n\n# There is a lot happening in this line, so let's break it down. The apply() command\n# will apply a function over a given data set, defined in the first argument. The 2 tell\n# apply() that it should apply the function by column. The third argument is an anonymous\n# function because we only use it this one time. We are going to ask if 'x' NA or not. We then\n# sum all the TRUE cases, telling us how many null values exist in each column of our data frame.\n# This is extremely useful when getting a sense of how 'dirty' your data is. Even if you don't\n# get what the code does right away, I recommend using this code in future debugging exercises.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1491930923160.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1554079650",
    "id" : "C6B70436",
    "lastKnownWriteTime" : 1491930993,
    "last_content_update" : 1491930993042,
    "path" : "C:/Users/yalot/OneDrive/CompSci/GitHub/Data-Science-in-Space-and-Time/Lab-01-Introduction/Lab-01-Skeleton.R",
    "project_path" : "Lab-01-Skeleton.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}